{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa39508f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Types of RNN\n",
    "1.one to one RNN\n",
    "2.Many to many Rnn\n",
    "3.many ton one RNN\n",
    "4.one to many RNN\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f76967d",
   "metadata": {},
   "source": [
    "\"\"\" bidirection rnn:\n",
    "\n",
    "Great question ğŸ‘ â€” bidirectional RNNs (BiRNNs) extend a normal RNN by processing the sequence **in both directions** (forward and backward), then merging the results.\n",
    "\n",
    "Letâ€™s break it down step by step.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¹ Forward Propagation Equations for Bidirectional RNN\n",
    "\n",
    "For a sequence $x = (x_1, x_2, ......, x_T)$:\n",
    "\n",
    "### 1. **Forward RNN pass**\n",
    "\n",
    "Processes the sequence left â†’ right:\n",
    "htâ†’â€‹=f(Wxhâ†’â€‹xtâ€‹+Whhâ†’â€‹htâˆ’1â†’â€‹+bhâ†’â€‹)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Backward RNN pass**\n",
    "\n",
    "Processes the sequence right â†’ left:\n",
    "\n",
    "htâ†â€‹=f(Wxhâ†â€‹xtâ€‹+Whhâ†â€‹ht+1â†â€‹+bhâ†â€‹)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Combining the two hidden states**\n",
    "\n",
    "At each time step $t$, we merge the forward and backward hidden states:\n",
    "\n",
    "htâ€‹=[htâ†’â€‹;htâ†â€‹]\n",
    "\n",
    "(Concatenation is common, but sometimes sum or average is used.)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Output Layer**\n",
    "\n",
    "For prediction/output at each time step:\n",
    "\n",
    "ytâ€‹=g(Whyâ€‹htâ€‹+byâ€‹)\n",
    "\n",
    "where:\n",
    "\n",
    "*\n",
    "f(â‹…)= activation for hidden state (usually tanh or ReLU)\n",
    "* ğ‘”(â‹…)= output activation (softmax for classification, sigmoid for binary, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¹ Summary\n",
    "\n",
    "* **Forward RNN:** depends on â„ğ‘¡âˆ’1â†’htâˆ’1â†’â€‹\n",
    "* **Backward RNN:** depends on ht+1â†â€‹\n",
    "* **Final hidden state:** concatenation of both directions\n",
    "* **Output:** computed from combined hidden state\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘‰ Would you like me to also derive the **LSTM version of bidirectional forward propagation equations** (with gates: input, forget, output, etc.), or keep it at the vanilla RNN level?\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
