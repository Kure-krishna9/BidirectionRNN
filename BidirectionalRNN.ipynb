{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa39508f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Types of RNN\n",
    "1.one to one RNN\n",
    "2.Many to many Rnn\n",
    "3.many ton one RNN\n",
    "4.one to many RNN\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f76967d",
   "metadata": {},
   "source": [
    "\"\"\" bidirection rnn:\n",
    "\n",
    "Great question 👍 — bidirectional RNNs (BiRNNs) extend a normal RNN by processing the sequence **in both directions** (forward and backward), then merging the results.\n",
    "\n",
    "Let’s break it down step by step.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Forward Propagation Equations for Bidirectional RNN\n",
    "\n",
    "For a sequence $x = (x_1, x_2, ......, x_T)$:\n",
    "\n",
    "### 1. **Forward RNN pass**\n",
    "\n",
    "Processes the sequence left → right:\n",
    "ht→​=f(Wxh→​xt​+Whh→​ht−1→​+bh→​)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Backward RNN pass**\n",
    "\n",
    "Processes the sequence right → left:\n",
    "\n",
    "ht←​=f(Wxh←​xt​+Whh←​ht+1←​+bh←​)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Combining the two hidden states**\n",
    "\n",
    "At each time step $t$, we merge the forward and backward hidden states:\n",
    "\n",
    "ht​=[ht→​;ht←​]\n",
    "\n",
    "(Concatenation is common, but sometimes sum or average is used.)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Output Layer**\n",
    "\n",
    "For prediction/output at each time step:\n",
    "\n",
    "yt​=g(Why​ht​+by​)\n",
    "\n",
    "where:\n",
    "\n",
    "*\n",
    "f(⋅)= activation for hidden state (usually tanh or ReLU)\n",
    "* 𝑔(⋅)= output activation (softmax for classification, sigmoid for binary, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 Summary\n",
    "\n",
    "* **Forward RNN:** depends on ℎ𝑡−1→ht−1→​\n",
    "* **Backward RNN:** depends on ht+1←​\n",
    "* **Final hidden state:** concatenation of both directions\n",
    "* **Output:** computed from combined hidden state\n",
    "\n",
    "---\n",
    "\n",
    "👉 Would you like me to also derive the **LSTM version of bidirectional forward propagation equations** (with gates: input, forget, output, etc.), or keep it at the vanilla RNN level?\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
